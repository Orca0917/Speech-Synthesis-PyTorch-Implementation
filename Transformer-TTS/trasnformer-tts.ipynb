{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from config import Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    # default\n",
    "    base_path = '../LJSpeech-1.1'\n",
    "    preprocess_base_path = base_path + '/preprocessed'\n",
    "\n",
    "    # preprocess\n",
    "    wav_paths = preprocess_base_path + '/paths'\n",
    "    mel_paths = preprocess_base_path + '/mels'\n",
    "    spec_paths = preprocess_base_path + '/specs'\n",
    "    transcript_paths = preprocess_base_path + '/transcripts'\n",
    "    phoneme_paths = preprocess_base_path + '/phonemes'\n",
    "\n",
    "    # metadata\n",
    "    data_path = base_path + '/wavs'\n",
    "    metadata_path = base_path + '/metadata.csv'\n",
    "\n",
    "    # train\n",
    "    batch_size = 16\n",
    "    \n",
    "\n",
    "    # model\n",
    "    num_phonemes = 70\n",
    "    num_mels = 80\n",
    "    embedding_dim = 512\n",
    "    d_model = 512\n",
    "    num_heads = 8\n",
    "    num_encoder_layers = 6\n",
    "    num_decoder_layers = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_specs(S, mel_S, sr):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.subplot(2, 1, 1)\n",
    "    librosa.display.specshow(S, sr=sr, x_axis='time', y_axis='log')\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    plt.title('Spectrogram')\n",
    "\n",
    "    plt.subplot(2, 1, 2)\n",
    "    librosa.display.specshow(mel_S, sr=sr, x_axis='time', y_axis='mel')\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    plt.title('Mel spectrogram')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerTTSDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.phoneme_to_index = {}\n",
    "        with open(Config.metadata_path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            self.wav_names = [line.split('|')[0] for line in lines]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.wav_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        wav_name = self.wav_names[idx]\n",
    "\n",
    "        phoneme = np.load(f'{Config.phoneme_paths}/{wav_name}.npy')\n",
    "        spectrogram = np.load(f'{Config.spec_paths}/{wav_name}.npy')\n",
    "        melspectrogram = np.load(f'{Config.mel_paths}/{wav_name}.npy')\n",
    "\n",
    "        # phoneme to index\n",
    "        for ph in phoneme:\n",
    "            if ph not in self.phoneme_to_index:\n",
    "                self.phoneme_to_index[ph] = len(self.phoneme_to_index) + 1\n",
    "\n",
    "        phoneme_seq = [self.phoneme_to_index[ph] for ph in phoneme]\n",
    "\n",
    "        phoneme_seq = torch.LongTensor(phoneme_seq)\n",
    "        spectrogram = torch.FloatTensor(spectrogram)\n",
    "        melspectrogram = torch.FloatTensor(melspectrogram)\n",
    "        \n",
    "        return (\n",
    "            phoneme_seq,\n",
    "            spectrogram,\n",
    "            melspectrogram\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequence1D(seq):\n",
    "    return nn.utils.rnn.pad_sequence(sequences=seq, batch_first=True, padding_value=0)\n",
    "\n",
    "\n",
    "def pad_sequence2D(seqs):\n",
    "    B                   = len(seqs)\n",
    "    T                   = len(seqs[0])\n",
    "    max_len             = max([len(seq[0]) for seq in seqs])\n",
    "    padded_mel          = torch.zeros(B, T, max_len, ) # 멜 스펙트로그램 차원 맞춰주기\n",
    "    for i, seq in enumerate(seqs):\n",
    "        # padded_mel[i, :, :] = -80.0\n",
    "        padded_mel[i, :seq.size(0), :seq.size(1)] = seq\n",
    "    return padded_mel\n",
    "\n",
    "\n",
    "def pad_sequence1D_stops(seqs):\n",
    "    for seq in seqs:\n",
    "        seq[-1] = 1\n",
    "    return nn.utils.rnn.pad_sequence(sequences=seqs, batch_first=True, padding_value=1)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    seqs = [item[0] for item in batch]\n",
    "    spectrograms = [item[1] for item in batch]\n",
    "    melspectrograms = [item[2] for item in batch]\n",
    "    stops = [torch.zeros_like(mel[0]) for mel in melspectrograms]\n",
    "\n",
    "    seq_lengths = torch.LongTensor([len(seq) for seq in seqs])\n",
    "    mel_lengths = torch.LongTensor([len(mel[0]) for mel in melspectrograms])\n",
    "\n",
    "    padded_seqs = pad_sequence1D(seqs)\n",
    "    padded_spectrograms = pad_sequence2D(spectrograms)\n",
    "    padded_melspectrograms = pad_sequence2D(melspectrograms)\n",
    "    padded_stop_seqs = pad_sequence1D_stops(stops)\n",
    "\n",
    "    return (\n",
    "        padded_seqs, \n",
    "        padded_spectrograms, \n",
    "        padded_melspectrograms, \n",
    "        padded_stop_seqs, \n",
    "        seq_lengths,\n",
    "        mel_lengths\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv1DBN(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, kernel_size, padding, act=None):\n",
    "        super(Conv1DBN, self).__init__()\n",
    "        self.conv_1d = nn.Conv1d(in_dim, out_dim, kernel_size=kernel_size, padding=padding)\n",
    "        self.bn = nn.BatchNorm1d(out_dim)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.activation = act\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_1d(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        if self.activation is not None:\n",
    "            x = self.activation(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Prenet(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim):\n",
    "        super(Prenet, self).__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.relu = nn.ReLU()\n",
    "        self.layers = nn.Sequential(\n",
    "            Conv1DBN(in_dim, hidden_dim, 5, 5//2, self.relu),\n",
    "            Conv1DBN(hidden_dim, hidden_dim, 5, 5//2, self.relu),\n",
    "            Conv1DBN(hidden_dim, out_dim, 5, 5//2, self.relu)\n",
    "        )\n",
    "        self.linear = nn.Linear(out_dim, out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.layers(x)\n",
    "        x = x.transpose(1, 2)\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledPositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(ScaledPositionalEncoding, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.max_len = max_len\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(np.log(10000.0) / d_model))\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "        self.alpha = nn.Parameter(torch.ones(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.alpha * self.pe[:, :x.size(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PostNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PostNet, self).__init__()\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.layers = nn.Sequential(\n",
    "            Conv1DBN(80, 512, 5, 5//2, self.tanh),\n",
    "            Conv1DBN(512, 512, 5, 5//2, self.tanh),\n",
    "            Conv1DBN(512, 512, 5, 5//2, self.tanh),\n",
    "            Conv1DBN(512, 512, 5, 5//2, self.tanh),\n",
    "            Conv1DBN(512, 80, 5, 5//2)\n",
    "        )\n",
    "        self.linear = nn.Linear(80, 80)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.layers(x)\n",
    "        x = x.transpose(1, 2)\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerTTS(nn.Module):\n",
    "   \n",
    "    def __init__(self):\n",
    "        super(TransformerTTS, self).__init__()\n",
    "        self.phoneme_embedding = nn.Embedding(num_embeddings=Config.num_phonemes, embedding_dim=Config.embedding_dim)\n",
    "        self.encoder_prenet = Prenet(Config.embedding_dim, 512, 512)\n",
    "        self.decoder_prenet = Prenet(Config.num_mels, 512, 512)\n",
    "        \n",
    "        self.scaled_positional_encoding = ScaledPositionalEncoding(Config.d_model)\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=Config.d_model,\n",
    "            num_encoder_layers=Config.num_encoder_layers,\n",
    "            num_decoder_layers=Config.num_decoder_layers,\n",
    "            nhead=Config.num_heads,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        \n",
    "        self.mel_linear = nn.Linear(Config.d_model, Config.num_mels)\n",
    "        self.stop_linear = nn.Linear(Config.d_model, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        self.postnet = PostNet()\n",
    "\n",
    "\n",
    "    def forward(self, phoneme_sequences, mel_spectrograms, phoneme_sequence_lengths=None, mel_spectrogram_lengths=None):\n",
    "        self.initialize_masks(x_lengths=phoneme_sequence_lengths, y_lengths=mel_spectrogram_lengths)\n",
    "\n",
    "        phoneme_embeddings = self.phoneme_embedding(phoneme_sequences)\n",
    "        print('phoneme을 512차원 embedding vector로 변환', phoneme_embeddings.shape)\n",
    "              \n",
    "        prenet_phoneme_embeddings = self.encoder_prenet(phoneme_embeddings)\n",
    "        print('encoder_prenet_out', prenet_phoneme_embeddings.shape)\n",
    "\n",
    "        positional_phoneme_embeddings = self.scaled_positional_encoding(prenet_phoneme_embeddings)\n",
    "        print('positional_encoded', positional_phoneme_embeddings.shape)\n",
    "\n",
    "        decoder_input_mel_spectrograms = mel_spectrograms.transpose(1, 2)\n",
    "        prenet_mel_spectrograms = self.decoder_prenet(decoder_input_mel_spectrograms)\n",
    "        print('decoder_prenet_out', prenet_mel_spectrograms.shape)\n",
    "\n",
    "        positional_mel_spectrograms = self.scaled_positional_encoding(prenet_mel_spectrograms)\n",
    "        print('positional_encoded', positional_mel_spectrograms.shape)\n",
    "        \n",
    "        transformer_out = self.transformer(\n",
    "            src=positional_phoneme_embeddings, \n",
    "            tgt=positional_mel_spectrograms,\n",
    "            src_mask=self.src_mask,\n",
    "            src_key_padding_mask=self.src_key_padding_mask,\n",
    "            tgt_mask=self.tgt_mask,\n",
    "            tgt_key_padding_mask=self.tgt_key_padding_mask,\n",
    "            memory_mask=self.memory_mask\n",
    "        )\n",
    "        print('transformer_out', transformer_out.shape)\n",
    "\n",
    "        mel_linear_out = self.mel_linear(transformer_out)\n",
    "        print('mel_linear_out', mel_linear_out.shape)\n",
    "\n",
    "        postnet_out = self.postnet(mel_linear_out)\n",
    "        print('postnet_out', postnet_out.shape)\n",
    "\n",
    "        mel_pred = mel_linear_out + postnet_out\n",
    "\n",
    "        stop_token = self.sigmoid(self.stop_linear(transformer_out))\n",
    "        print('stop_token', stop_token.shape)\n",
    "        \n",
    "        mel_pred = mel_pred.transpose(1, 2)\n",
    "        stop_token = stop_token.squeeze(-1)\n",
    "\n",
    "        # mel_pred에서 seq_len을 넘어가는 부분은 모두 -80.0 으로 채워준다.\n",
    "        # for i, seq_len in enumerate(x_legnths):\n",
    "        #     mel_pred[i, :, seq_len:] = -80.0\n",
    "\n",
    "        mel_pred.data.masked_fill_(\n",
    "            self.tgt_key_padding_mask.unsqueeze(-1).repeat(1, 1, mel_pred.size(-1)), 0.0)\n",
    "        stop_token.data.masked_fill_(\n",
    "            self.tgt_key_padding_mask.unsqueeze(-1), 1e3)\n",
    "\n",
    "        return mel_pred, stop_token\n",
    "    \n",
    "\n",
    "    def generate_square_subsequent_mask(self, lsz, rsz):\n",
    "        return torch.triu(torch.ones(lsz, rsz) * float('-inf'), diagonal=1)\n",
    "    \n",
    "\n",
    "    def generate_padding_mask(self, lengths, max_len=None):\n",
    "        batch_size = lengths.size(0)\n",
    "        if max_len is None:\n",
    "            max_len = torch.max(lengths).item()\n",
    "        ids = torch.arange(0, max_len).unsqueeze(0).expand(batch_size, -1).to(dtype=lengths.dtype, device=lengths.device)\n",
    "        return ids >= lengths.unsqueeze(1).expand(-1, max_len)\n",
    "    \n",
    "    def initialize_masks(self, x_lengths=None, y_lengths=None):\n",
    "        self.src_mask = None\n",
    "        self.tgt_mask = None\n",
    "        self.memory_mask = None\n",
    "        self.src_key_padding_mask = None\n",
    "        self.tgt_key_padding_mask = None\n",
    "        if x_lengths is not None:\n",
    "            S = x_lengths.max().item()\n",
    "            self.src_mask = self.generate_square_subsequent_mask(S, S).to(device=x_lengths.device)         # text sequence self-attention mask\n",
    "            self.src_key_padding_mask = self.generate_padding_mask(x_lengths).to(device=x_lengths.device)  # text sequence padding mask\n",
    "        if y_lengths is not None:\n",
    "            T = y_lengths.max().item()\n",
    "            self.tgt_mask = self.generate_square_subsequent_mask(T, T).to(device=y_lengths.device)         # mel sequence self-attention mask\n",
    "            self.tgt_key_padding_mask = self.generate_padding_mask(y_lengths).to(device=y_lengths.device)  # mel sequence padding mask\n",
    "        if x_lengths is not None and y_lengths is not None:\n",
    "            T = y_lengths.max().item()\n",
    "            S = x_lengths.max().item()\n",
    "            self.memory_mask = self.generate_square_subsequent_mask(T, S).to(device=y_lengths.device)      # text-mel cross attention mask\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerTTSLoss():\n",
    "    def __init__(self):\n",
    "        self.mel_loss = nn.MSELoss()\n",
    "        self.stop_loss = nn.BCELoss()\n",
    "        self.alpha = 5.0\n",
    "\n",
    "    def __call__(self, mel_pred, mel_target, stop_pred, stop_target):\n",
    "        mel_loss = self.mel_loss(mel_pred, mel_target)\n",
    "        stop_loss = self.stop_loss(stop_pred, stop_target)\n",
    "        return mel_loss + stop_loss * self.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3275\n",
      "phoneme torch.Size([4, 116]), spec: torch.Size([4, 1025, 354]), mel: torch.Size([4, 80, 354]), stop: torch.Size([4, 354]), phoneme_len: tensor([116,  78, 109, 114]), mel_len: tensor([354, 287, 344, 339])\n",
      "tensor([116,  78, 109, 114])\n",
      "- model information\n",
      "phoneme을 512차원 embedding vector로 변환 torch.Size([4, 116, 512])\n",
      "encoder_prenet_out torch.Size([4, 116, 512])\n",
      "positional_encoded torch.Size([4, 116, 512])\n",
      "decoder_prenet_out torch.Size([4, 354, 512])\n",
      "positional_encoded torch.Size([4, 354, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/moon/opt/anaconda3/envs/transformer-tts/lib/python3.12/site-packages/torch/nn/functional.py:5137: UserWarning: Support for mismatched src_key_padding_mask and mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "/Users/moon/opt/anaconda3/envs/transformer-tts/lib/python3.12/site-packages/torch/nn/functional.py:5137: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer_out torch.Size([4, 354, 512])\n",
      "mel_linear_out torch.Size([4, 354, 80])\n",
      "postnet_out torch.Size([4, 354, 80])\n",
      "stop_token torch.Size([4, 354, 1])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (80) must match the size of tensor b (354) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[68], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(phoneme_len)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m- model information\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 17\u001b[0m mel_pred, stop_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mphoneme\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mphoneme_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmel_len\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(mel_pred, mel, stop_pred, stop)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/transformer-tts/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/transformer-tts/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[66], line 73\u001b[0m, in \u001b[0;36mTransformerTTS.forward\u001b[0;34m(self, phoneme_sequences, mel_spectrograms, phoneme_sequence_lengths, mel_spectrogram_lengths)\u001b[0m\n\u001b[1;32m     67\u001b[0m stop_token \u001b[38;5;241m=\u001b[39m stop_token\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# mel_pred에서 seq_len을 넘어가는 부분은 모두 -80.0 으로 채워준다.\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# for i, seq_len in enumerate(x_legnths):\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m#     mel_pred[i, :, seq_len:] = -80.0\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m \u001b[43mmel_pred\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmasked_fill_\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtgt_key_padding_mask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepeat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmel_pred\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m stop_token\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mmasked_fill_(\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtgt_key_padding_mask\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), \u001b[38;5;241m1e3\u001b[39m)\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m mel_pred, stop_token\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (80) must match the size of tensor b (354) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "transformer_tts_dataset = TransformerTTSDataset()\n",
    "dataloader = DataLoader(transformer_tts_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "model = TransformerTTS()\n",
    "criterion = TransformerTTSLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "\n",
    "print(len(dataloader))\n",
    "\n",
    "# 데이터로더에서 데이터를 가져와서 확인하기\n",
    "for i, data in enumerate(dataloader):\n",
    "    phoneme, spec, mel, stop, phoneme_len, mel_len = data\n",
    "    print(f'phoneme {phoneme.shape}, spec: {spec.shape}, mel: {mel.shape}, stop: {stop.shape}, phoneme_len: {phoneme_len}, mel_len: {mel_len}')\n",
    "    print(phoneme_len)\n",
    "\n",
    "    print(\"- model information\")\n",
    "    mel_pred, stop_pred = model(phoneme, mel, phoneme_len, mel_len)\n",
    "\n",
    "    loss = criterion(mel_pred, mel, stop_pred, stop)\n",
    "    print(f'loss: {loss.item()}')\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if i % 2 == 0:\n",
    "        visualize_specs(mel_pred[0].detach().numpy(), mel[0].detach().numpy(), sr=22050)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "        [0., 0., -inf,  ..., -inf, -inf, -inf],\n",
      "        [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., -inf, -inf],\n",
      "        [0., 0., 0.,  ..., 0., 0., -inf],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "def generate_square_subsequent_mask(self, size=200): # Generate mask covering the top right triangle of a matrix\n",
    "        mask = torch.triu(torch.full((size, size), float('-inf')), diagonal=1)\n",
    "        return mask\n",
    "\n",
    "print(generate_square_subsequent_mask(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader, Dataset\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BertTokenizer\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Sample Dataset\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mCustomDataset\u001b[39;00m(Dataset):\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# Sample Dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.tokenizer(self.data[idx], return_tensors='pt', padding=True, truncation=True)\n",
    "\n",
    "# Collate function to handle variable lengths and padding\n",
    "def collate_fn(batch):\n",
    "    input_ids = [item['input_ids'].squeeze(0) for item in batch]\n",
    "    attention_masks = [item['attention_mask'].squeeze(0) for item in batch]\n",
    "\n",
    "    input_ids = nn.utils.rnn.pad_sequence(input_ids, batch_first=True, padding_value=0)\n",
    "    attention_masks = nn.utils.rnn.pad_sequence(attention_masks, batch_first=True, padding_value=0)\n",
    "\n",
    "    return input_ids, attention_masks\n",
    "\n",
    "# Sample data\n",
    "data = [\"Hello, how are you?\", \"I am fine, thank you!\", \"What about you?\"]\n",
    "\n",
    "# DataLoader\n",
    "dataset = CustomDataset(data)\n",
    "dataloader = DataLoader(dataset, batch_size=2, collate_fn=collate_fn)\n",
    "\n",
    "# Transformer model with masked attention\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, max_seq_length):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.positional_encoding = nn.Parameter(torch.zeros(1, max_seq_length, d_model))\n",
    "        self.transformer = nn.Transformer(d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward)\n",
    "        self.fc = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
    "        src = self.embedding(src) + self.positional_encoding[:, :src.size(1), :]\n",
    "        tgt = self.embedding(tgt) + self.positional_encoding[:, :tgt.size(1), :]\n",
    "        output = self.transformer(src, tgt, src_key_padding_mask=src_mask, tgt_key_padding_mask=tgt_mask)\n",
    "        return self.fc(output)\n",
    "\n",
    "# Function to create masks\n",
    "def create_mask(src, tgt):\n",
    "    src_seq_len = src.shape[1]\n",
    "    tgt_seq_len = tgt.shape[1]\n",
    "\n",
    "    src_mask = (src != 0).unsqueeze(1).unsqueeze(2)  # (batch_size, 1, 1, src_seq_len)\n",
    "    tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3)  # (batch_size, 1, tgt_seq_len, 1)\n",
    "\n",
    "    return src_mask, tgt_mask\n",
    "\n",
    "# Initialize model, loss, and optimizer\n",
    "vocab_size = 30522  # Vocabulary size of BERT tokenizer\n",
    "d_model = 512\n",
    "nhead = 8\n",
    "num_encoder_layers = 3\n",
    "num_decoder_layers = 3\n",
    "dim_feedforward = 2048\n",
    "max_seq_length = 50\n",
    "\n",
    "model = TransformerModel(vocab_size, d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, max_seq_length)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    for batch in dataloader:\n",
    "        input_ids, attention_masks = batch\n",
    "        tgt_input = input_ids[:, :-1]\n",
    "        tgt_output = input_ids[:, 1:]\n",
    "\n",
    "        src_mask, tgt_mask = create_mask(input_ids, tgt_input)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(input_ids, tgt_input, src_mask, tgt_mask)\n",
    "\n",
    "        loss = criterion(output.view(-1, vocab_size), tgt_output.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer-tts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
