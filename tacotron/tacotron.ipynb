{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jtm7mMZYZ4lE"
      },
      "source": [
        "# Tacotron\n",
        "\n",
        "- Tacotron은 2017년도에 arxiv에 발표된 논문으로, (음성, 텍스트) 쌍으로 부터 spectrogram을 생성해주는 모델이다.\n",
        "- 이번 구현에서 사용할 데이터 셋은 LJSpeech 데이터셋으로 단일 화자가 녹음했으며 총 24시간 분량이다. (영어)\n",
        "- 논문 링크: [Tacotron: Towards End-to-End Speech Synthesis](https://arxiv.org/abs/1703.10135)\n",
        "- 블로그 링크: https://orca0917.github.io/posts/Tacotron/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3mQ87b1WJkG",
        "outputId": "0856506a-3f26-4e43-dcf5-f1866a43ba44"
      },
      "outputs": [],
      "source": [
        "# 필요한 모듈 설치\n",
        "%pip install g2p_en"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oN2IOqh_oS5v"
      },
      "source": [
        "# 1. 데이터셋 다운로드\n",
        "\n",
        "- LJSpeech dataset: https://keithito.com/LJ-Speech-Dataset/\n",
        "- 크기: 약 2.6GB\n",
        "- 데이터셋 형식: `tar.bz2` 확장자로 압축, 내부 음원은 `.wav` 확장자"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "id": "8S6_bFFEdzVc",
        "outputId": "b72a05cf-a76c-45bb-d792-924dbdf0fa7c"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from tqdm.notebook import tqdm\n",
        "import tarfile\n",
        "import os\n",
        "\n",
        "def download_and_extract(dataset_url, extract_to='.'):\n",
        "    # 파일명 추출\n",
        "    filename = dataset_url.split('/')[-1]\n",
        "    filepath = os.path.join(extract_to, filename)\n",
        "\n",
        "    # 데이터셋 다운로드\n",
        "    print(f\"다운로드 중 {filename}...\")\n",
        "    response = requests.get(dataset_url, stream=True)\n",
        "    total_size_in_bytes = 2750000000\n",
        "    block_size = 1024 # 1 Kibibyte\n",
        "    progress_bar = tqdm(total=total_size_in_bytes, unit='iB', unit_scale=True, leave=True)\n",
        "    with open(filepath, 'wb') as file:\n",
        "        for data in response.iter_content(block_size):\n",
        "            progress_bar.update(len(data), )\n",
        "            file.write(data)\n",
        "    progress_bar.close()\n",
        "\n",
        "    # 압축 해제\n",
        "    print(f\"압축 해제 중 {filename}...\")\n",
        "    if filepath.endswith(\"tar.bz2\"):\n",
        "        tar = tarfile.open(filepath, \"r:bz2\")\n",
        "        tar.extractall(path=extract_to)\n",
        "        tar.close()\n",
        "    else:\n",
        "        print(\"알 수 없는 압축 형식입니다.. 오직 .tar.bz2 파일만 압축 해제할 수 있습니다.\")\n",
        "\n",
        "    # 다운로드한 파일 삭제\n",
        "    os.remove(filepath)\n",
        "    print(f\"{filename} 는 압축해제가 성공적으로 완료되었으며 다운로드 된 파일은 삭제하였습니다.\")\n",
        "\n",
        "# LJSpeech 데이터셋 URL (실제 URL을 사용하세요)\n",
        "dataset_url = 'https://data.keithito.com/data/speech/LJSpeech-1.1.tar.bz2'\n",
        "# 압축을 풀 폴더 지정\n",
        "extract_to = '/content/'\n",
        "\n",
        "# 함수 실행\n",
        "download_and_extract(dataset_url, extract_to)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVb3etk9otW5"
      },
      "source": [
        "# 2. 커스텀 데이터셋 정의\n",
        "\n",
        "- 커스텀 데이터셋으로 부터 (텍스트, 정답 스펙트로그램) 입력쌍을 사용\n",
        "- 각 입력마다 들어가는 텍스트의 길이, 음원의 길이 모두 다르기 때문에 `collate_fn` 함수 정의 필요"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FSIZb-BXrNxy",
        "outputId": "85df249b-dbc1-4b27-ccd4-03cc49def114"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import string\n",
        "import librosa\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from g2p_en import G2p\n",
        "# from google.colab import drive\n",
        "from tqdm import tqdm\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f'현재 사용중인 장치 = {device}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ys6D2psumCfJ"
      },
      "outputs": [],
      "source": [
        "args = {\n",
        "    # 데이터셋 파라미터\n",
        "    'batch_size' : 16,\n",
        "    'n_mels': 80,\n",
        "    'lin_dim': 1025,\n",
        "    'wav_path': '/opt/ml/LJSpeech-1.1/wavs/',\n",
        "    'metadata_path': '/opt/ml/LJSpeech-1.1/metadata.csv',\n",
        "    'pre_emphasis': 0.97,\n",
        "    'sr': 24000,\n",
        "    'frame_length': 0.050,\n",
        "    'frame_shift': 0.0125,\n",
        "    'n_fft': 2048,\n",
        "\n",
        "    # 인코더 파라미터\n",
        "    'n_vocab' : len(string.punctuation) + len(G2p().phonemes) + 2,\n",
        "    'char_embedding_dim' : 256,\n",
        "    'prenet_dims' : [256, 128],\n",
        "    'enc_n_kernel' : 16,\n",
        "    'enc_in_dim' : 128,\n",
        "    'enc_cbhg_projection_dims' : [128, 128],\n",
        "\n",
        "    # 디코더 파라미터\n",
        "    'r' : 1,\n",
        "    'dec_hidden_dim' : 256,\n",
        "    'dec_n_kernel' : 8,\n",
        "    'dec_cbhg_projection_dims' : [256, 80]\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lsyL9V4tKFB2"
      },
      "outputs": [],
      "source": [
        "class LJSpeechDataset(Dataset):\n",
        "    \"\"\"\n",
        "    학습에 사용될 데이터를 준비해주는 클래스\n",
        "    - LJSpeech 데이터셋을 사용하여 총 3개의 값을 반환\n",
        "    - wav 파일의 이름, wav 파일의 대본(텍스트), wav 파일의 스펙트로그램\n",
        "    - wav 파일의 이름은 확장자를 포함하지 않는다.\n",
        "    \"\"\"\n",
        "    def __init__(self, args):\n",
        "        super(LJSpeechDataset, self)\n",
        "        self.metadata_path  = args['metadata_path']\n",
        "        self.pre_emphasis   = args['pre_emphasis']\n",
        "        self.frame_length   = args['frame_length']\n",
        "        self.frame_shift    = args['frame_shift']\n",
        "        self.wav_path       = args['wav_path']\n",
        "        self.n_mels         = args['n_mels']\n",
        "        self.n_fft          = args['n_fft']\n",
        "        self.sr             = args['sr']\n",
        "        self.g2p            = G2p()\n",
        "\n",
        "        # 음소와 문장부호를 숫자로 매핑해주는 dict\n",
        "        self.symbols = list(string.punctuation) + self.g2p.phonemes + [' ']\n",
        "        self.symbol_to_id = {s: i + 1 for i, s in enumerate(self.symbols)}\n",
        "\n",
        "        # 음성파일 이름과 대본들\n",
        "        metadata_df         = pd.read_csv(self.metadata_path, delimiter='|', header=None)\n",
        "        self.wav_file_names = metadata_df[0].values\n",
        "        self.text_script    = metadata_df[2].values\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.wav_file_names)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        sequence        = self.get_sequence(self.text_script[index])\n",
        "        spec, melspec   = self.get_spectrograms(self.wav_file_names[index])\n",
        "        return sequence, spec, melspec\n",
        "\n",
        "    # 텍스트를 음소로 변환하고, 음소를 숫자로 변환하여 반환하는 함수\n",
        "    def get_sequence(self, text):\n",
        "        phonemes = self.g2p(text)\n",
        "        ret = []\n",
        "        for p in phonemes:\n",
        "            if p not in self.symbols: # 항상 phoneme 리스트에 존재하는 값들만 다룸\n",
        "                continue\n",
        "            ret.append(self.symbol_to_id[p])\n",
        "        return np.array(ret)\n",
        "\n",
        "    # 음성파일로 부터 선형 스펙트로그램을 구하는 함수\n",
        "    def get_spectrograms(self, wav_file_name):\n",
        "        y, sr = librosa.load(os.path.join(self.wav_path, wav_file_name + '.wav'), sr=self.sr)\n",
        "        y_preemphasized = np.append(y[0], y[1:] - self.pre_emphasis * y[:-1])\n",
        "\n",
        "        # 스펙트로그램을 생성하기 위한 STFT(Short-Time Fourier Transform) 계산\n",
        "        hop_length = int(sr * self.frame_shift)\n",
        "        win_length = int(sr * self.frame_length)\n",
        "\n",
        "        D = librosa.stft(y=y_preemphasized,\n",
        "                        n_fft=self.n_fft,\n",
        "                        hop_length=hop_length,\n",
        "                        win_length=win_length,\n",
        "                        window='hann')\n",
        "        S = librosa.amplitude_to_db(np.abs(D), ref=np.max)\n",
        "\n",
        "        # 멜 스펙트로그램 계산\n",
        "        mel_S = librosa.feature.melspectrogram(S=D,\n",
        "                                               sr=sr,\n",
        "                                               n_fft=self.n_fft,\n",
        "                                               hop_length=hop_length,\n",
        "                                               win_length=win_length,\n",
        "                                               n_mels=self.n_mels)\n",
        "        mel_S = librosa.amplitude_to_db(np.abs(mel_S), ref=np.max)\n",
        "\n",
        "        return S, mel_S\n",
        "\n",
        "\n",
        "# 배치마다 존재하는 음성의 길이가 다르므로 패딩 처리 해주기 (가장 긴 것에 맞추기)\n",
        "def collate_fn(batch):\n",
        "    # 텍스트\n",
        "    max_seq_len = max([len(item[0]) for item in batch])\n",
        "    pad_seq = np.array(\n",
        "        [np.pad(item[0], (0, max_seq_len - len(item[0])), mode='constant', constant_values=0) for item in batch],\n",
        "        dtype=np.int32)\n",
        "\n",
        "    # 스펙트로그램\n",
        "    max_spec_len = max([item[1].shape[1] for item in batch])\n",
        "    pad_spec = np.array(\n",
        "        [np.pad(item[1], [(0, 0), (0, max_spec_len - item[1].shape[1])], mode='constant', constant_values=0) for item in batch],\n",
        "        dtype=np.float32\n",
        "    )\n",
        "\n",
        "    # 멜스펙트로그램\n",
        "    max_melspec_len = max([item[2].shape[1] for item in batch])\n",
        "    pad_melspec = np.array(\n",
        "        [np.pad(item[2], [(0, 0), (0, max_melspec_len - item[2].shape[1])], mode='constant', constant_values=0) for item in batch],\n",
        "        dtype=np.float32\n",
        "    )\n",
        "\n",
        "    pad_seq     = torch.LongTensor(pad_seq)\n",
        "    pad_spec    = torch.FloatTensor(pad_spec).transpose(1, 2)\n",
        "    pad_melspec = torch.FloatTensor(pad_melspec).transpose(1, 2)\n",
        "\n",
        "    return pad_seq, pad_spec, pad_melspec\n",
        "\n",
        "\n",
        "dataset = LJSpeechDataset(args)\n",
        "dataloader = DataLoader(dataset,\n",
        "                        batch_size=args['batch_size'],\n",
        "                        shuffle=False,\n",
        "                        num_workers=2,\n",
        "                        collate_fn=collate_fn,)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WeY-GQbbpEXF"
      },
      "source": [
        "# 3. Tacotron 모델 구현\n",
        "- Encoder\n",
        "- Decoder\n",
        "- Griffin Lim 보코더로 구성\n",
        "\n",
        "![](https://orca0917.github.io/assets/img/tacotron/tacotron-figure1.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YeW7YGKPJZRD"
      },
      "source": [
        "## 3.1. Encoder 구현\n",
        "\n",
        "- Text Embedding\n",
        "- PreNet\n",
        "- CBHG (Convolution Bank, Highway Network, GRU)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XU7rSmt_fZ-E"
      },
      "outputs": [],
      "source": [
        "class HighwayNet(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.H = nn.Linear(dim, dim)\n",
        "        self.T = nn.Linear(dim, dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.sigmoid = nn.Sigmoid()  # dim 확인해보기\n",
        "\n",
        "    def forward(self, x):\n",
        "        C = self.sigmoid(self.T(x))\n",
        "        y = self.relu(self.H(x))\n",
        "        return (1 - C) * x + C * y\n",
        "\n",
        "\n",
        "class BNConv1D(nn.Module):\n",
        "    \"\"\"\n",
        "    1D Convolution과 함께 Batch Normalization을 적용해주는 클래스\n",
        "    \"\"\"\n",
        "    def __init__(self, in_dim, out_dim, kernel_size, stride, padding):\n",
        "        super(BNConv1D, self).__init__()\n",
        "        self.conv1d = nn.Conv1d(in_dim, out_dim, kernel_size, stride, padding)\n",
        "        self.bn = nn.BatchNorm1d(out_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1d(x)\n",
        "        x = self.bn(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class CBHG(nn.Module):\n",
        "    \"\"\"\n",
        "    CBHG는 1D 컨볼루션 필터 여러개와 함께, highway network, GRU로 구성된 모듈이다.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_dim, K, proj_dims):\n",
        "        super(CBHG, self).__init__()\n",
        "        self.conv_bank = nn.ModuleList([\n",
        "            BNConv1D(in_dim, in_dim, k + 1, 1, k // 2) for k in range(K)])\n",
        "        self.max_pool = nn.MaxPool1d(kernel_size=2, stride=1, padding=1)\n",
        "\n",
        "        p_ins  = [in_dim * K] + proj_dims[:-1]\n",
        "        p_outs = proj_dims\n",
        "\n",
        "        self.projections = nn.ModuleList([\n",
        "            BNConv1D(p_in, p_out, 3, 1, 1) for p_in, p_out in zip(p_ins, p_outs)])\n",
        "\n",
        "        self.activations = nn.ModuleList([\n",
        "            nn.ReLU() for _ in range(len(p_outs) - 1)] + [None])\n",
        "\n",
        "        self.proj_linear  = nn.Linear(p_outs[-1], p_outs[-1], bias=False)\n",
        "        self.highway      = nn.ModuleList([HighwayNet(in_dim) for _ in range(4)])\n",
        "        self.bi_gru       = nn.GRU(in_dim, in_dim, 1, batch_first=True, bidirectional=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x                        # (B, L, 128)\n",
        "        x = x.transpose(1, 2)               # (B, 128, L)\n",
        "        L = x.shape[-1]\n",
        "\n",
        "        conv_result = []\n",
        "        for conv in self.conv_bank:\n",
        "            y = conv(x)\n",
        "            y = F.pad(y, (0, L - y.shape[-1]))\n",
        "            conv_result.append(y)\n",
        "\n",
        "        x = torch.cat(conv_result, dim=1)   # (B, 128 * 16, L)\n",
        "        x = self.max_pool(x)[:, :, :L]      # (B, 128 * 16, L)\n",
        "\n",
        "        for proj, act in zip(self.projections, self.activations):\n",
        "            x = proj(x)                     # (B, 128, L)\n",
        "            if act is not None:\n",
        "                x = act(x)\n",
        "        x = x.transpose(1, 2)               # (B, L, 128)\n",
        "        x = self.proj_linear(x)             # (B, L, 128)\n",
        "\n",
        "        # residual connection\n",
        "        x += residual                       # (B, L, 128)\n",
        "\n",
        "        for highway in self.highway:\n",
        "            x = highway(x)        # (B, L, 128)\n",
        "\n",
        "        x, _ = self.bi_gru(x)               # (B, L, 128 * 2)\n",
        "        return x\n",
        "\n",
        "\n",
        "class PreNet(nn.Module):\n",
        "    def __init__(self, in_dim):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(in_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layers(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super(Encoder, self).__init__()\n",
        "        cbhg_proj   = args['enc_cbhg_projection_dims']\n",
        "        c_emb_dim   = args['char_embedding_dim']\n",
        "        cbhg_k      = args['enc_n_kernel']\n",
        "        in_dim      = args['enc_in_dim']\n",
        "        n_vocab     = args['n_vocab']\n",
        "\n",
        "        self.char_embedding = nn.Embedding(n_vocab, c_emb_dim)\n",
        "        self.enc_prenet     = PreNet(in_dim=c_emb_dim)\n",
        "        self.cbhg           = CBHG(in_dim=in_dim, K=cbhg_k, proj_dims=cbhg_proj)\n",
        "\n",
        "    def forward(self, x: torch.LongTensor):\n",
        "                                    # (B, L)\n",
        "        x = self.char_embedding(x)  # (B, L, 256)\n",
        "        x = self.enc_prenet(x)      # (B, L, 128)\n",
        "        x = self.cbhg(x)            # (B, L, 128 * 2)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhyiNUFdRVJ1"
      },
      "source": [
        "## 3.2. Decoder 구현"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CIDAr14wJ2dm"
      },
      "outputs": [],
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Attention, self).__init__()\n",
        "        self.enc_W  = nn.Linear(256, 256)\n",
        "        self.dec_W  = nn.Linear(256, 256)\n",
        "        self.v      = nn.Linear(256, 1, bias=False)\n",
        "        self.tanh   = nn.Tanh()\n",
        "\n",
        "    def forward(self, encoder_outputs, gru_output):\n",
        "        enc_out = self.enc_W(encoder_outputs)               # (B, 138, 256)\n",
        "        gru_out = self.dec_W(gru_output)                    # (B, 256)\n",
        "\n",
        "        gru_out = gru_out.unsqueeze(1)                      # (B, 1, 256)\n",
        "        alignment = self.v(self.tanh(enc_out + gru_out))    # (B, 138, 1)\n",
        "        alignment = alignment.squeeze(-1)                   # (B, 138)\n",
        "        alignment = F.softmax(alignment, dim=1)             # (B, 138)\n",
        "        return alignment\n",
        "\n",
        "\n",
        "class AttentionRNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(AttentionRNN, self).__init__()\n",
        "        self.gru_cell = nn.GRUCell(128 + 256, 256)\n",
        "        self.attention = Attention()\n",
        "\n",
        "    def forward(self, rnn_input, cell_hidden, attn_hidden, encoder_outputs):\n",
        "        \"\"\"\n",
        "        - rnn_input : (B, 128)\n",
        "        - cell_hidden : (B, 256)\n",
        "        - attn_hidden : (B, 256)\n",
        "        - encoder_outputs : (B, 138, 256)\n",
        "        \"\"\"\n",
        "\n",
        "        cell_input = torch.cat((rnn_input, attn_hidden), dim=-1)            # (B, 128 + 256)\n",
        "        cell_output = self.gru_cell(cell_input, cell_hidden)                # (B, 256)\n",
        "        alignment = self.attention(encoder_outputs, cell_output)            # (B, 138)\n",
        "\n",
        "        #-- 여기서부터는 논문에 언급된 내용이 없었음 (참고: r9y9)\n",
        "        attn_hidden = torch.bmm(alignment.unsqueeze(1), encoder_outputs)    # (B, 1, 256)\n",
        "        attn_hidden = attn_hidden.squeeze(1)                                # (B, 256)\n",
        "\n",
        "        return cell_output, attn_hidden, alignment\n",
        "\n",
        "\n",
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(512, 256, bias=False)\n",
        "        self.grus = nn.ModuleList([\n",
        "            nn.GRUCell(256, 256) for _ in range(2)\n",
        "        ])\n",
        "\n",
        "    def forward(self, cell_hidden, attn_hidden, dec_rnn_hiddens):\n",
        "        \"\"\"\n",
        "        - cell_hidden : (B, 256)\n",
        "        - attn_hidden : (B, 256)\n",
        "        - dec_rnn_hiddens : (2, B, 256)\n",
        "        \"\"\"\n",
        "        decoder_in = torch.cat((cell_hidden, attn_hidden), dim=-1)  # (B, 512)\n",
        "        decoder_in = self.linear(decoder_in)                        # (B, 256)\n",
        "        for rnn_hidden, gru in zip(dec_rnn_hiddens, self.grus):\n",
        "            rnn_hidden = gru(decoder_in, rnn_hidden)                # (B, 256)\n",
        "            decoder_in = rnn_hidden + decoder_in                    # (B, 256)\n",
        "\n",
        "        return decoder_in                                           # (B, 256)\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.r              = args['r']\n",
        "        self.mel_dim        = args['n_mels']\n",
        "        self.linear_dim     = args['lin_dim']\n",
        "        self.dec_hidden_dim = args['dec_hidden_dim']\n",
        "        self.dec_prenet     = PreNet(in_dim=self.mel_dim * self.r)\n",
        "        self.attention_rnn  = AttentionRNN()\n",
        "        self.decoder_rnn    = DecoderRNN()\n",
        "        self.projection     = nn.Linear(self.dec_hidden_dim, self.mel_dim * self.r)\n",
        "        self.dec_cbhg       = CBHG(in_dim=self.mel_dim,\n",
        "                                   K=args['dec_n_kernel'],\n",
        "                                   proj_dims=args['dec_cbhg_projection_dims'])\n",
        "        self.to_linear      = nn.Linear(self.mel_dim * 2, self.linear_dim)\n",
        "\n",
        "    def forward(self, enc_out, T):\n",
        "        # enc_out  (B, T, 256)\n",
        "        B = enc_out.shape[0]\n",
        "\n",
        "        # 초기 hidden state 및 attention 들\n",
        "        go_frame        =  torch.zeros(B, self.mel_dim * self.r, dtype=torch.float32).to(device)  # <go> frame\n",
        "        cell_hidden     =  torch.zeros(B, self.dec_hidden_dim, dtype=torch.float32).to(device)\n",
        "        attn_hidden     =  torch.zeros(B, self.dec_hidden_dim, dtype=torch.float32).to(device)\n",
        "        dec_rnn_hiddens = [torch.zeros(B, self.dec_hidden_dim, dtype=torch.float32).to(device) for _ in range(2)]\n",
        "\n",
        "        # 예측한 alignment와 mel_frame을 담아두는 곳\n",
        "        alignments, mel_frames = [], []\n",
        "\n",
        "        for step in range(T):\n",
        "            if step > 0:\n",
        "                go_frame = mel_frames[-1]\n",
        "            prenet_out = self.dec_prenet(go_frame)                                                                      # (B, 128)\n",
        "            cell_hidden, attn_hidden, alignment = self.attention_rnn(prenet_out, cell_hidden, attn_hidden, enc_out)\n",
        "            dec_out = self.decoder_rnn(cell_hidden, attn_hidden, dec_rnn_hiddens)                                       # (B, 256)\n",
        "\n",
        "            mel_frame = self.projection(dec_out)                                                                        # (B, 80 * 3)\n",
        "            alignments += [alignment]\n",
        "            mel_frames += [mel_frame]\n",
        "\n",
        "        alignments = torch.stack(alignments).transpose(0, 1)                                                            # (B, T * r, )\n",
        "        mel_frames = torch.stack(mel_frames).transpose(0, 1).contiguous()\n",
        "\n",
        "        mel_frames = mel_frames.view(B, -1, self.mel_dim)                                                               # (B, T * r, 80)\n",
        "        spectrogram = self.dec_cbhg(mel_frames)                                                                         # (B, T * r, 128 * 2)\n",
        "        spectrogram = self.to_linear(spectrogram)                                                                       # (B, r * step, 1025)\n",
        "        return alignments, mel_frames, spectrogram\n",
        "\n",
        "class Tacotron(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super(Tacotron, self).__init__()\n",
        "        self.encoder = Encoder(args)\n",
        "        self.decoder = Decoder()\n",
        "\n",
        "    def forward(self, x, T):\n",
        "        # (B, L)\n",
        "        x = self.encoder(x)  # (B, L, 128 * 2)\n",
        "        alignments, melspectrogram, spectrogram = self.decoder(x, T)\n",
        "        return alignments, melspectrogram, spectrogram"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VithzLeO6X87"
      },
      "source": [
        "# 모델 실험\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "7b6287141f3a4428ad12a5535deaeb4c",
            "7ea674b9074d4417a6c1f444a5dfd5b4",
            "e475ab1af10a4b44bf293c8190c9cbb9",
            "20b92f9c464b40af8f6dbe741313caf4",
            "a9e24480791b49d498f70902f7375b9c",
            "0d535e0a2a3c4509ba44c32341de9a80",
            "047955558e22471da58cb5395af4a0f5",
            "0499d6cb96824564baabe0621ab293a0",
            "c27b339b553a4ccabcceb128b668d6db",
            "39a28fcb758c4b8fa2ae4fdd25c84d78",
            "ccaa94ec1e584d41aa2be78c78fffe72"
          ]
        },
        "id": "kmPly-vmP8yx",
        "outputId": "4d74c9c0-45c3-408e-98b2-7412a2587bb8"
      },
      "outputs": [],
      "source": [
        "model = Tacotron(args)\n",
        "criterion = nn.L1Loss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        "model = model.to(device)\n",
        "loss_history = []\n",
        "\n",
        "for epoch in range(1):\n",
        "\n",
        "    global_loss = []\n",
        "\n",
        "    for i, (x, y_lin, y_mel) in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
        "        # print(f'데이터 타입: {x.dtype}, 크기: {x.shape}') # (B, L)\n",
        "        # print(f'데이터 타입: {y_lin.dtype}, 크기: {y_lin.shape}') # (B, 1025, T)\n",
        "        # print(f'데이터 타입: {y_mel.dtype}, 크기: {y_mel.shape}') # (B, 80, T)\n",
        "\n",
        "        x       = x.to(device)\n",
        "        y_lin   = y_lin.to(device)\n",
        "        y_mel   = y_mel.to(device)\n",
        "\n",
        "        T = y_lin.shape[1]\n",
        "        alignment, pred_mel, pred_lin = model(x, T)\n",
        "\n",
        "        # print(f'데이터 타입 <alignment>: {alignment.dtype}, 크기: {alignment.shape}')\n",
        "        # print(f'데이터 타입 <melspectrogram>: {pred_mel.dtype}, 크기: {pred_mel.shape}')\n",
        "        # print(f'데이터 타입 <spectrogram>: {pred_lin.dtype}, 크기: {pred_lin.shape}')\n",
        "\n",
        "        mel_loss = criterion(pred_mel, y_mel)\n",
        "        lin_loss = criterion(pred_lin, y_lin)\n",
        "        loss = mel_loss + lin_loss\n",
        "        global_loss += [loss.item()]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    loss_history += [np.mean(global_loss)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygUxJfkRsxyz"
      },
      "source": [
        "# Loss 값 비교"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tyABGm3cs3ac"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(loss_history)\n",
        "plt.title('Loss convergence', fontsize=20, fontweight='semibold')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "oN2IOqh_oS5v"
      ],
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "047955558e22471da58cb5395af4a0f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0499d6cb96824564baabe0621ab293a0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0d535e0a2a3c4509ba44c32341de9a80": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "20b92f9c464b40af8f6dbe741313caf4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_39a28fcb758c4b8fa2ae4fdd25c84d78",
            "placeholder": "​",
            "style": "IPY_MODEL_ccaa94ec1e584d41aa2be78c78fffe72",
            "value": " 539/1638 [26:57&lt;49:48,  2.72s/it]"
          }
        },
        "39a28fcb758c4b8fa2ae4fdd25c84d78": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7b6287141f3a4428ad12a5535deaeb4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7ea674b9074d4417a6c1f444a5dfd5b4",
              "IPY_MODEL_e475ab1af10a4b44bf293c8190c9cbb9",
              "IPY_MODEL_20b92f9c464b40af8f6dbe741313caf4"
            ],
            "layout": "IPY_MODEL_a9e24480791b49d498f70902f7375b9c"
          }
        },
        "7ea674b9074d4417a6c1f444a5dfd5b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0d535e0a2a3c4509ba44c32341de9a80",
            "placeholder": "​",
            "style": "IPY_MODEL_047955558e22471da58cb5395af4a0f5",
            "value": " 33%"
          }
        },
        "a9e24480791b49d498f70902f7375b9c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c27b339b553a4ccabcceb128b668d6db": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ccaa94ec1e584d41aa2be78c78fffe72": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e475ab1af10a4b44bf293c8190c9cbb9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0499d6cb96824564baabe0621ab293a0",
            "max": 1638,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c27b339b553a4ccabcceb128b668d6db",
            "value": 539
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
